# Bert-finetuned-text-classification
## Project Motivations
[BERT](https://github.com/google-research/bert) is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. The BERT model was pretrained on BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers).The model was formally pre-trained for two major objectives; Masked Language Modelling(MLM) and Next Sentence Prediction (NSP). However, the model has recently been used for text classification tasks that includes sentiment analysis. The idea of this project is to explore the sequence classification by using bert checkpoint (bert-base-uncased) and fine tune this model on General Language Understanding Evalautuion (GLUE) datasets of Stanford Sentiment Treebank (sst2) that consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence.
 It uses the two-way (positive/negative) class split, with only sentence-levellabels. The metrics of the finetuned model shows high significant of the model performance on text classification on sentiment analysis.
